How and when to use different Activation functions:-



1.Linear

The simplest case, no activation, the signal goes through unchanged. This is a good choice for regression problems.


2.ReLU/ELU ----- max(0,x)
ReLU is one of the most widely used Activation function and gives an output of X if X is positive and 0 otherwise.


3.Softmax  ----- ğ‘’ğ‘¥ğ‘–âˆ‘ğ‘–ğ‘’ğ‘¥ğ‘–exiâˆ‘iexi
Softmax function is always used in the output layers that generates the output between 0 and 1.

4.Tanh     -----  ğ‘§âˆ’1ğ‘’ğ‘§+1ezâˆ’1ez+1
Tanh is used where we want to take output from [-1,1] range covered all scenrios .
In some cases the sign of the output is relevant, but the magnitude can mess with the further computations. 
Tanh is an elegant way to â€œsquishâ€ the output into [âˆ’1,1][âˆ’1,1] range, preserving the sign and conforming to 
the boundary conditions ğ‘“(0)=0,ğ‘“â€²(âˆ)=0f(0)=0,fâ€²(âˆ)=0. Itâ€™s useful when after the magnitude of unprocessed output 
grows significantly, the further growth is not that important, and vice versa, 
when the fluctuations around zero make significant difference.


5.Sigmoid
A Sigmoid function is often used in the hidden layers where you are trying to predict the probability as an output (0 or 1).
